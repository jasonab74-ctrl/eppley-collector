name: Run Eppley Scraper

on:
  workflow_dispatch:
  # 3 / 5 / 7 AM UTC (as shown on your site)
  schedule:
    - cron: '0 3,5,7 * * *'

permissions:
  contents: write   # allow this job to push CSV updates

concurrency:
  group: eppley-scraper
  cancel-in-progress: false

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        env:
          PYTHONUNBUFFERED: '1'
        run: |
          python main.py

      # ---- FIXED COMMIT STEP (rebase + retry) ----
      - name: Commit results
        run: |
          set -e
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"

          # sync with the latest main to avoid "fetch first" rejections
          git fetch origin main
          git checkout main
          git pull --rebase origin main

          git add -A
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi
          git commit -m "Auto-update scraped data $(date -u '+%Y-%m-%d %H:%M:%S')"

          # push; if rejected (race), rebase once more and push again
          git push origin main || (git pull --rebase origin main && git push origin main)