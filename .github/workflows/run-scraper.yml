name: Run Eppley Scraper (Resumable, Chunked, Safe)

on:
  schedule:
    - cron: "0 3 * * *"
    - cron: "0 5 * * *"
    - cron: "0 7 * * *"
  workflow_dispatch:

permissions:
  contents: write
  issues: write

concurrency:
  group: eppley-scraper
  cancel-in-progress: true

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 420
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade --force-reinstall -r requirements.txt

      # WordPress (resumable, chunked)
      - name: WP chunk 1
        run: python main.py --only wp --wp-mode auto --wp-max 600 --save-every 150 --delay 3.0
      - name: Commit chunk 1
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add output/
          git commit -m "WP chunk 1 $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push
      - name: WP chunk 2
        run: python main.py --only wp --wp-mode auto --wp-max 600 --save-every 150 --delay 3.0
      - name: Commit chunk 2
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add output/
          git commit -m "WP chunk 2 $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push

      # APIs (each isolated so one failure doesn't kill the rest)
      - name: PubMed
        run: python main.py --only pubmed
      - name: Crossref
        run: python main.py --only crossref
      - name: OpenAlex
        run: python openalex_collect.py
      - name: ClinicalTrials.gov
        run: python main.py --only ct
      - name: ORCID
        run: python main.py --only orcid
      - name: YouTube (search + channels)
        run: python youtube_collect.py
      - name: Semantic Scholar
        run: python s2_collect.py

      - name: Merge publications
        run: python merge_publications.py

      - name: Final commit
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add output/
          git commit -m "Auto-update scraped data $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push

      - name: Mark health failed
        if: failure()
        run: |
          mkdir -p output
          echo '{"last_status":"failed","generated_at":"'"$(date -u +%FT%TZ)"'"}' > output/health.json
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add output/health.json
          git commit -m "health: failed" || true
          git push || true

      - name: Create issue on failure
        if: failure()
        uses: dacbd/create-issue-action@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          title: "ðŸ”´ Scraper failed â€” ${{ github.workflow }} @ run ${{ github.run_id }}"
          body: |
            **Workflow:** ${{ github.workflow }}
            **Run:** https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
            **Branch:** ${{ github.ref }}
            The scraper encountered an error. Please review the run logs.

      - name: Upload partial output on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: partial-output
          path: output/**