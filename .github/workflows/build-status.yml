name: Build Data Status

on:
  schedule:
    - cron: "20 3 * * *"
    - cron: "20 5 * * *"
    - cron: "20 7 * * *"
  workflow_dispatch: {}
  workflow_run:
    workflows: ["Run Eppley Scraper (Resumable, Chunked, Safe)"]
    types: [completed]

permissions:
  contents: write

jobs:
  build-status:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Generate output/status.json (auto-discover CSVs)
        run: |
          python - <<'PY'
          import os, csv, json, glob
          from datetime import datetime, timezone

          def raw_url(path):
              return f"https://raw.githubusercontent.com/${{ github.repository }}/main/{path}"
          def blob_url(path):
              return f"https://github.com/${{ github.repository }}/blob/main/{path}"

          status_path = "output/status.json"
          prev = {}
          if os.path.exists(status_path):
              try:
                  with open(status_path,"r",encoding="utf-8") as f: prev = json.load(f)
              except Exception: prev = {}
          prev_map = { (r.get("name") or r.get("path")): r for r in prev.get("files", []) } if isinstance(prev, dict) else {}

          files = sorted(glob.glob("output/*.csv"))
          label_map = {
              "wordpress_posts.csv":"All blog+Q&A posts",
              "pubmed_eppley.csv":"Publications (PubMed)",
              "youtube_metadata.csv":"YouTube metadata",
              "crossref_works.csv":"Publications (Crossref)",
              "openalex_works.csv":"Publications & metrics (OpenAlex)",
              "clinical_trials.csv":"ClinicalTrials.gov studies",
              "orcid_profiles.csv":"ORCID profiles",
              "orcid_works.csv":"ORCID public works",
              "semanticscholar_works.csv":"Publications (Semantic Scholar)",
              "publications_all.csv":"Merged publications (deduped)"
          }

          rows_out = []
          updated_at = datetime.now(timezone.utc).isoformat(timespec="seconds")
          for path in files:
              name = os.path.basename(path)
              entry = {
                  "name": name, "label": label_map.get(name, "Data"),
                  "path": path, "exists": os.path.exists(path),
                  "updated_at": updated_at, "size_bytes": 0, "size_mb": 0.0,
                  "rows": 0, "new_rows_since_last_run": 0,
                  "raw_url": raw_url(path), "webpage_url": blob_url(path), "download_url": raw_url(path)
              }
              if entry["exists"]:
                  entry["size_bytes"] = os.path.getsize(path)
                  entry["size_mb"] = round(entry["size_bytes"]/(1024*1024),3)
                  try:
                      with open(path,"r",encoding="utf-8",errors="ignore") as f:
                          reader = csv.reader(f); rows = list(reader)
                          if rows: rows = rows[1:]  # skip header
                          entry["rows"] = len(rows)
                  except Exception:
                      pass
                  prior = prev_map.get(name) or prev_map.get(path) or {}
                  try:
                      entry["new_rows_since_last_run"] = max(0, entry["rows"] - int(prior.get("rows", 0)))
                  except Exception:
                      entry["new_rows_since_last_run"] = 0

              rows_out.append(entry)

          out = { "repo": "${{ github.repository }}", "generated_at": updated_at, "files": rows_out }
          os.makedirs("output", exist_ok=True)
          with open(status_path,"w",encoding="utf-8") as f: json.dump(out, f, ensure_ascii=False, indent=2)
          print(json.dumps(out, indent=2))
          PY

      - name: Write health.json (ok)
        run: |
          mkdir -p output
          echo '{"last_status":"ok","generated_at":"'"$(date -u +%FT%TZ)"'"}' > output/health.json

      - name: Generate manifest.json (checksums)
        run: |
          python - <<'PY'
          import hashlib, json, glob, os, time
          m=[]
          for p in sorted(glob.glob("output/*.csv")):
              h=hashlib.sha256()
              with open(p,'rb') as f:
                  for chunk in iter(lambda:f.read(1<<20), b''): h.update(chunk)
              m.append({"name":os.path.basename(p),"sha256":h.hexdigest(),"bytes":os.path.getsize(p)})
          out={"generated_at":time.strftime("%Y-%m-%dT%H:%M:%SZ",time.gmtime()),"files":m}
          os.makedirs("output", exist_ok=True)
          with open("output/manifest.json","w") as f: json.dump(out,f,indent=2)
          PY

      - name: Commit status + health + manifest
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add output/status.json output/health.json output/manifest.json
          git commit -m "Update data status & health & manifest" || echo "No changes"
          git push

      - name: Mark health failed (if any step failed)
        if: failure()
        run: |
          mkdir -p output
          echo '{"last_status":"failed","generated_at":"'"$(date -u +%FT%TZ)"'"}' > output/health.json
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add output/health.json
          git commit -m "health: failed" || true
          git push || true